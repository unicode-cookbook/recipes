{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of grapheme segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steven Moran &lt;bambooforest@gmail.com&gt;\n",
    "\n",
    "The latest version of this [Jupyter notebook](http://jupyter.org/) is available at [https://github.com/unicode-cookbook/recipes/Basics](https://github.com/unicode-cookbook/recipes/Basics). \n",
    "\n",
    "This use case illustrates how to segment text into graphemes. We also transliterate graphemes using an orthography profile. Details about orthography profiles and more is available in the [Unicode Cookbook for Linguists](https://github.com/unicode-cookbook/cookbook).\n",
    "\n",
    "This recipe uses Python 3.5. \n",
    "\n",
    "Github renders Jupyter notebooks nicely, so you can copy and paste code into your interpreter or scripts. If you however `git clone` the `recipes` repository and have Jupyter installed on your machine, this file is also executable in the browser. Run `jupyter notebook` in this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Python [segments](https://pypi.python.org/pypi/segments/) package to tokenize characters, graphemes and IPA. Installation instructures here: [https://github.com/unicode-cookbook/recipes](https://github.com/unicode-cookbook/recipes). We illustrate both API access and the command line program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segments.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `characters` function will segment a string at Unicode code points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c ̂ h a ́ ɾ a ̃ ̌ c t ʼ ɛ ↗ ʐ ː | # k ͡ p\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "result = t.characters(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grapheme_clusters` function will segment text at the [Unicode Extended Grapheme Cluster](http://www.unicode.org/reports/tr18/tr18-19.html#Default_Grapheme_Clusters) boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĉ h á ɾ ã̌ c t ʼ ɛ ↗ ʐ ː | # k͡ p\n"
     ]
    }
   ],
   "source": [
    "result = t.grapheme_clusters(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grapheme_clusters` function is the default segmentation algorithm for the `segments.Tokenizer`. It is useful when you encounter a text that you want to tokenize to identify orthographic or transcription elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĉ h á ɾ ã̌ c t ʼ ɛ ↗ ʐ ː | # k͡ p\n"
     ]
    }
   ],
   "source": [
    "result = t(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ipa` parameter forces grapheme segmentation for [IPA strings](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĉ h á ɾ ã̌ c tʼ ɛ ↗ ʐː | # k͡p\n"
     ]
    }
   ],
   "source": [
    "result = t(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\", ipa=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load an orthography profile and tokenize an input string with it. In the `data` directory we've placed an example orthograpy profile. Let's have a look at it using `more` on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Grapheme\\tIPA\\tXSAMPA\\tCOMMENT',\n",
       " 'a\\ta\\ta',\n",
       " 'aa\\taː\\ta:',\n",
       " 'b\\tb\\tb',\n",
       " 'c\\tc\\tc',\n",
       " 'ch\\ttʃ\\ttS',\n",
       " '-\\tNULL\\tNULL\\t\"comment with\\ttab\"',\n",
       " 'on\\tõ\\to~',\n",
       " 'n\\tn\\tn',\n",
       " 'ih\\tí\\ti_H',\n",
       " 'inh\\tĩ́\\ti~_H']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more data/orthography-profile.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pipe this to `csvlook` for nicer display in Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['| Grapheme | IPA | XSAMPA | COMMENT          |',\n",
       " '| -------- | --- | ------ | ---------------- |',\n",
       " '| a        | a   | a      |                  |',\n",
       " '| aa       | aː  | a:     |                  |',\n",
       " '| b        | b   | b      |                  |',\n",
       " '| c        | c   | c      |                  |',\n",
       " '| ch       | tʃ  | tS     |                  |',\n",
       " '| -        |     |        | comment with\\ttab |',\n",
       " '| on       | õ  | o~     |                  |',\n",
       " '| n        | n   | n      |                  |',\n",
       " '| ih       | í  | i_H    |                  |',\n",
       " '| inh      | ĩ́ | i~_H   |                  |']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more data/orthography-profile.tsv | csvlook -t"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "An orthograpy profile is delimited UTF-8 text file (here we use tab as a delimter for reading ease). The first column must be labelled `Grapheme`. Each row in the `Grapheme` column specifies graphemes that may be found in the orthography of the input text. In this example, we provide additional columns `IPA` and `XSAMPA`, which are mappings from our graphemes to their [IPA](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet) and [XSAMPA](https://en.wikipedia.org/wiki/X-SAMPA) transliterations. The final column `COMMENT` is for comments; if you want to use a tab ''quote that&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;string''!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the orthography profile with our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segments.tokenizer import Profile\n",
    "\n",
    "t = Tokenizer('data/orthography-profile.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's segment the graphemes in some input strings with our orthography profile. The output is segmented given the definition of graphemes in our orthography profile, e.g. we specified the sequence of two &lt;a a&gt; should be a single unit &lt;aa&gt;, and so should the sequences &lt;c h&gt;, &lt;o n&gt; and &lt;i h&gt;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa b ch on n - ih'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t('aabchonn-ih')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how can we tokenize input text into our orthograpic specification. We can also segment graphemes and transliterate them into other formats, which is useful when you have sources with different orthographies, but you want to be able to compare them using a single representation like IPA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aː b tʃ õ n í'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.transform(\"aabchonn-ih\", \"IPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a: b tS o~ n i_H'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.transform(\"aabchonn-ih\", \"XSAMPA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also useful to know which characters in your input string are not in your orthography profile. Use the function `find_missing_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa b ch on n - ih � � �'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.find_missing_characters(\"aa b ch on n - ih x y z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the default as the [Unicode replacement character \\ufffd](http://www.fileformat.info/info/unicode/char/fffd/index.htm). But you can simply change this by specifying the replacement character when you load the orthography profile with the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa b ch on n - ih ? ? ?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer('data/orthography-profile.tsv', errors_replace=lambda c: '?')\n",
    "t.find_missing_characters(\"aa b ch on n - ih x y z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa b ch on n - ih <x> <y> <z>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer('data/orthography-profile.tsv', errors_replace=lambda c: '<{0}>'.format(c))\n",
    "t.find_missing_characters(\"aa b ch on n - ih x y z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you want to create an initial orthography profile that also contains those graphemes x, y, z? Note that the space character and its frequency are also captured in this initial profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grapheme\tfrequency\tmapping\n",
      " \t9\t \n",
      "a\t2\ta\n",
      "h\t2\th\n",
      "n\t2\tn\n",
      "b\t1\tb\n",
      "c\t1\tc\n",
      "o\t1\to\n",
      "-\t1\t-\n",
      "i\t1\ti\n",
      "x\t1\tx\n",
      "y\t1\ty\n",
      "z\t1\tz\n"
     ]
    }
   ],
   "source": [
    "profile = Profile.from_text(\"aa b ch on n - ih x y z\")\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Command line access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to `pip install segments` to install the command line tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some help with `segments -h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['usage: segments [-h] [--verbosity VERBOSITY] [--encoding ENCODING]',\n",
       " '                [--profile PROFILE] [--mapping MAPPING]',\n",
       " '                command ...',\n",
       " '',\n",
       " 'Main command line interface of the segments package.',\n",
       " '',\n",
       " 'positional arguments:',\n",
       " '  command               tokenize | profile',\n",
       " '  args',\n",
       " '',\n",
       " 'optional arguments:',\n",
       " '  -h, --help            show this help message and exit',\n",
       " '  --verbosity VERBOSITY',\n",
       " '                        increase output verbosity',\n",
       " '  --encoding ENCODING   input encoding',\n",
       " '  --profile PROFILE     path to an orthography profile',\n",
       " '  --mapping MAPPING     column name in ortho profile to map graphemes',\n",
       " '',\n",
       " \"Use 'segments help <cmd>' to get help about individual commands.\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!segments -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created some test data in the `source/german.txt` file using the word 'Schächtelchen', which is the diminuitive form of 'Schachtel', meaning 'box, packet, or carton' in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Schächtelchen']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more sources/german.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an initial orthography profile of the German text by passing it to the `segments profile` command. The initial profile tokenizes the text on Unicode grapheme clusters, lists the frequency of each grapheme, and provides an initial mapping column by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['| Grapheme | frequency | mapping |',\n",
       " '| -------- | --------- | ------- |',\n",
       " '| c        |         3 | c       |',\n",
       " '| h        |         3 | h       |',\n",
       " '| e        |         2 | e       |',\n",
       " '| S        |         1 | S       |',\n",
       " '| ä       |         1 | ä      |',\n",
       " '| t        |         1 | t       |',\n",
       " '| l        |         1 | l       |',\n",
       " '| n        |         1 | n       |']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/german.txt | segments profile | csvlook -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we know a bit about German orthography and which characters combine to form German graphemes. We can use the information from our initial orthography profile to hand-curate a more precise German orthography profile that takes into account capitalization (German orthography obligatorily capitalizes nouns) and grapheme clusters, such as $<$sch$>$ and $<$ch$>$. We can use the initial orthography profile above as a starting point (note the in large text the frequency column may signal errors in the input, such as typos, if they occur with a very low frequency). The initial orthography profile can be edited with a text editor or spreadsheet program. As per the orthography profile specifications (see [Chapter 7](https://github.com/unicode-cookbook/cookbook/blob/master/unicode-cookbook.pdf)), we can adjust rows in the `Grapheme` column and then add additional columns for translitation or for comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['| Grapheme | IPA | XSAMPA | COMMENT                      |',\n",
       " '| -------- | --- | ------ | ---------------------------- |',\n",
       " '| Sch      | ʃ   | S      | German nouns are capitalized |',\n",
       " '| ä        | ɛː  | E:     |                              |',\n",
       " '| ch       | ç   | C      |                              |',\n",
       " '| t        | t   | t      |                              |',\n",
       " '| e        | e   | e      |                              |',\n",
       " '| l        | l   | l      |                              |',\n",
       " '| n        | n   | n      |                              |']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more data/german-orthography-profile.tsv | csvlook -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command line `segments` function and passing it our orthography profile, we can now segment our German text example into graphemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sch ä ch t e l ch e n']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/german.txt | segments --profile=data/german-orthography-profile.tsv tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by providing `segments` a column for transliteration, we can convert the text into IPA or XSAMPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ʃ ɛː ç t e l ç e n']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/german.txt | segments --mapping=IPA --profile=data/german-orthography-profile.tsv tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S E: C t e l C e n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/german.txt | segments --mapping=XSAMPA --profile=data/german-orthography-profile.tsv tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we provide another example of working with orthography profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aäaaöaaüaa']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more sources/text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['| Grapheme | frequency | mapping |',\n",
       " '| -------- | --------- | ------- |',\n",
       " '| a        |         7 | a       |',\n",
       " '| ä       |         1 | ä      |',\n",
       " '| ö       |         1 | ö      |',\n",
       " '| ü       |         1 | ü      |']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/text.txt | segments profile | csvlook -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/text.txt | segments profile > sandbox/orthography-profile.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Grapheme\\tfrequency\\tmapping',\n",
       " 'a\\t7\\ta',\n",
       " 'ä\\t1\\tä',\n",
       " 'ö\\t1\\tö',\n",
       " 'ü\\t1\\tü']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more sandbox/orthography-profile.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a ä a a ö a a ü a a']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/text.txt | segments tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a ä a a ö a a ü a a']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/text.txt | segments --mapping=mapping --profile=sandbox/orthography-profile.tsv tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aäaaöaaüaa']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more sources/text.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
