{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of grapheme tokenization and transliteration in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Steven Moran](http://www.comparativelinguistics.uzh.ch/de/moran.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latest version of this [Jupyter notebook](http://jupyter.org/) is available at [https://github.com/unicode-cookbook/recipes/Basics](https://github.com/unicode-cookbook/recipes/Basics). \n",
    "\n",
    "This use case illustrates how to segment text into graphemes. We also transliterate graphemes using an orthography profile. Details about orthography profiles and more is available in the [Unicode Cookbook for Linguists](https://github.com/unicode-cookbook/cookbook).\n",
    "\n",
    "This recipe uses Python 3.5. \n",
    "\n",
    "GitHub renders Jupyter notebooks nicely, so you can copy and paste code into your interpreter or scripts. If you however `git clone` this [recipes repository](https://github.com/unicode-cookbook/recipes) and have Jupyter installed on your machine, this file is also executable in the browser. Run `jupyter notebook` in this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Python [segments](https://pypi.python.org/pypi/segments/) package to tokenize graphemes and to transliterate input data with orthography profiles. Installation instructures here: [https://github.com/unicode-cookbook/recipes](https://github.com/unicode-cookbook/recipes). Examples from both the API and from the command line are shown in this recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing the `Tokenizer` from the `segments` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segments.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, instantiate a tokenizer object, which takes optional arguments for an orthography profile and an orthography profile rules files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default tokenization strategy is to segment some input text at the [Unicode Extended Grapheme Cluster](http://www.unicode.org/reports/tr18/tr18-19.html#Default_Grapheme_Clusters) boundaries, and to return, by default, a space-delimited string of graphemes. White space between input string sequences is by default separated by a hash symbol <#>, which is a linguistic convention used to denote word boundaries. The default grapheme tokenization is useful when you encounter a text that you want to tokenize to identify potential orthographic or transcription elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĉ h á ɾ ã̌ c t ʼ ɛ ↗ ʐ ː | # k͡ p\n"
     ]
    }
   ],
   "source": [
    "result = t(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĉ-h-á-ɾ-ã̌-c-t-ʼ-ɛ-↗-ʐ-ː-| # k͡-p\n"
     ]
    }
   ],
   "source": [
    "result = t(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\", segment_separator=\"-\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĉ h á ɾ ã̌ c t ʼ ɛ ↗ ʐ ː | // k͡ p\n"
     ]
    }
   ],
   "source": [
    "result = t(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\", separator=\" // \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optional `ipa` parameter forces grapheme segmentation for [IPA strings](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet). Note here that [Unicode Spacing Modifier Letters](https://en.wikipedia.org/wiki/Spacing_Modifier_Letters), such as <ː> and <◌͡◌>, will be segmented together with base characters (although you might need orthography profiles and rules to correct these in your input source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĉ h á ɾ ã̌ c tʼ ɛ ↗ ʐː | # k͡p\n"
     ]
    }
   ],
   "source": [
    "result = t(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\", ipa=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to tokenize a string at [Unicode code point](https://unicode.org/glossary/#code_point) boundaries, say for example to get the frequency of each and every character in your input, pass the tokenizer `characters=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c ̂ h a ́ ɾ a ̃ ̌ c t ʼ ɛ ↗ ʐ ː | # k ͡ p\n"
     ]
    }
   ],
   "source": [
    "# Forthcoming\n",
    "# result = t(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\", characters=True)\n",
    "# Current\n",
    "result = t.characters(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthography profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also load an orthography profile and tokenize input strings with it. In the `data` directory we've placed an example orthograpy profile. Let's have a look at it using `more` on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Grapheme\\tIPA\\tXSAMPA\\tCOMMENT',\n",
       " 'a\\ta\\ta',\n",
       " 'aa\\taː\\ta:',\n",
       " 'b\\tb\\tb',\n",
       " 'c\\tc\\tc',\n",
       " 'ch\\ttʃ\\ttS',\n",
       " '-\\tNULL\\tNULL\\t\"comment with\\ttab\"',\n",
       " 'on\\tõ\\to~',\n",
       " 'n\\tn\\tn',\n",
       " 'ih\\tí\\ti_H',\n",
       " 'inh\\tĩ́\\ti~_H']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more data/orthography-profile.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a slightly nicer display of Jupyter Notebooks on GitHub, we pipe our [TSV file](https://en.wikipedia.org/wiki/Tab-separated_values) to `csvlook` in [csvkit](http://csvkit.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['| Grapheme | IPA | XSAMPA | COMMENT          |',\n",
       " '| -------- | --- | ------ | ---------------- |',\n",
       " '| a        | a   | a      |                  |',\n",
       " '| aa       | aː  | a:     |                  |',\n",
       " '| b        | b   | b      |                  |',\n",
       " '| c        | c   | c      |                  |',\n",
       " '| ch       | tʃ  | tS     |                  |',\n",
       " '| -        |     |        | comment with\\ttab |',\n",
       " '| on       | õ  | o~     |                  |',\n",
       " '| n        | n   | n      |                  |',\n",
       " '| ih       | í  | i_H    |                  |',\n",
       " '| inh      | ĩ́ | i~_H   |                  |']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more data/orthography-profile.tsv | csvlook -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An orthograpy profile is a character delimited UTF-8 text file. The first column of the orthography profile must be labeled `Grapheme`. Other columns are optional. The formal specifcation is in defined in [Chapter 8](https://github.com/unicode-cookbook/cookbook/blob/master/unicode-cookbook.pdf) of The Unicode Cookbook for Linguists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the `Grapheme` column specifies graphemes that may be found in the orthography of the input text. In this example, we provide additional columns `IPA` and `XSAMPA`, which are mappings from our graphemes to their [IPA](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet) and [XSAMPA](https://en.wikipedia.org/wiki/X-SAMPA) transliterations. The final column `COMMENT` is for comments; if you want to use a tab, then ''quote that&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;string''!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the orthography profile with our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"IPA\" in table None\n",
      "  'Unspecified column \"{0}\" in table {1}'.format(k, self.local_name))\n",
      "/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"XSAMPA\" in table None\n",
      "  'Unspecified column \"{0}\" in table {1}'.format(k, self.local_name))\n",
      "/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"COMMENT\" in table None\n",
      "  'Unspecified column \"{0}\" in table {1}'.format(k, self.local_name))\n"
     ]
    }
   ],
   "source": [
    "from segments.tokenizer import Profile\n",
    "\n",
    "t = Tokenizer('data/orthography-profile.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's segment the graphemes in some input strings with our orthography profile. The output is segmented given the definition of graphemes in our orthography profile, e.g. we specified the sequence of two &lt;a a&gt; should be a single unit &lt;aa&gt;, and so should the sequences &lt;c h&gt;, &lt;o n&gt; and &lt;i h&gt;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa b ch on n - ih'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t('aabchonn-ih')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how can we tokenize input text into our orthographic specification. We can also segment graphemes and transliterate them into other formats, which we find useful when we have sources with different orthographies. Of course we also want to sometimes be able to compare different data sources in a single orthographic representation, like IPA or XSAMPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aː b tʃ õ n í'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(\"aabchonn-ih\", column=\"IPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a: b tS o~ n i_H'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(\"aabchonn-ih\", column=\"XSAMPA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also useful to know which characters in your input data are not in your orthography profile. By default, missing characters are displayed with the [Unicode REPLACEMENT CHARACTER U+FFFD](http://www.fileformat.info/info/unicode/char/fffd/index.htm), which appears below as a white question mark within a black diamond <�>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa # b # ch # on # n # - # ih # � # � # �'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(\"aa b ch on n - ih x y z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the default by specifying a different replacement character when you load the orthography profile with the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa # b # ch # on # n # - # ih # ? # ? # ?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer('data/orthography-profile.tsv', errors_replace=lambda c: '?')\n",
    "t(\"aa b ch on n - ih x y z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa # b # ch # on # n # - # ih # <x> # <y> # <z>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer('data/orthography-profile.tsv', errors_replace=lambda c: '<{0}>'.format(c))\n",
    "t(\"aa b ch on n - ih x y z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you want to create an initial orthography profile that also contains those graphemes &lt;x&gt;, &lt;y&gt;, and &lt;z&gt;? (Note that the space character and its frequency are also captured in this initial profile.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grapheme\tmapping\tfrequency\r\n",
      " \t \t9\r\n",
      "a\ta\t2\r\n",
      "h\th\t2\r\n",
      "n\tn\t2\r\n",
      "b\tb\t1\r\n",
      "c\tc\t1\r\n",
      "o\to\t1\r\n",
      "-\t-\t1\r\n",
      "i\ti\t1\r\n",
      "x\tx\t1\r\n",
      "y\ty\t1\r\n",
      "z\tz\t1\n"
     ]
    }
   ],
   "source": [
    "profile = Profile.from_text(\"aa b ch on n - ih x y z\")\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Command line access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to `pip install segments` to install the command line tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some help with `segments -h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['usage: segments [-h] [--verbosity VERBOSITY] [--encoding ENCODING]',\n",
       " '                [--profile PROFILE] [--mapping MAPPING]',\n",
       " '                command ...',\n",
       " '',\n",
       " 'Main command line interface of the segments package.',\n",
       " '',\n",
       " 'positional arguments:',\n",
       " '  command               tokenize | profile',\n",
       " '  args',\n",
       " '',\n",
       " 'optional arguments:',\n",
       " '  -h, --help            show this help message and exit',\n",
       " '  --verbosity VERBOSITY',\n",
       " '                        increase output verbosity',\n",
       " '  --encoding ENCODING   input encoding',\n",
       " '  --profile PROFILE     path to an orthography profile',\n",
       " '  --mapping MAPPING     column name in ortho profile to map graphemes',\n",
       " '',\n",
       " \"Use 'segments help <cmd>' to get help about individual commands.\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!segments -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created some test data in the `source/german.txt` file using the word 'Schächtelchen', which is the diminuitive form of 'Schachtel', meaning 'box, packet, or carton' in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Schächtelchen']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more sources/german.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an initial orthography profile of the German text by passing it to the `segments profile` command. The initial profile tokenizes the text on Unicode grapheme clusters, lists the frequency of each grapheme, and provides an initial mapping column by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['| Grapheme | frequency | mapping |',\n",
       " '| -------- | --------- | ------- |',\n",
       " '| c        |         3 | c       |',\n",
       " '| h        |         3 | h       |',\n",
       " '| e        |         2 | e       |',\n",
       " '| S        |         1 | S       |',\n",
       " '| ä        |         1 | ä       |',\n",
       " '| t        |         1 | t       |',\n",
       " '| l        |         1 | l       |',\n",
       " '| n        |         1 | n       |']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/german.txt | segments profile | csvlook -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we know a bit about German orthography and which characters combine to form German graphemes. We can use the information from our initial orthography profile to hand-curate a more precise German orthography profile that takes into account capitalization (German orthography obligatorily capitalizes nouns) and grapheme clusters, such as $<$sch$>$ and $<$ch$>$. We can use the initial orthography profile above as a starting point (note the in large text the frequency column may signal errors in the input, such as typos, if they occur with a very low frequency). The initial orthography profile can be edited with a text editor or spreadsheet program. As per the orthography profile specifications (see [Chapter 7](https://github.com/unicode-cookbook/cookbook/blob/master/unicode-cookbook.pdf)), we can adjust rows in the `Grapheme` column and then add additional columns for translitation or for comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['| Grapheme | IPA | XSAMPA | COMMENT                      |',\n",
       " '| -------- | --- | ------ | ---------------------------- |',\n",
       " '| Sch      | ʃ   | S      | German nouns are capitalized |',\n",
       " '| ä        | ɛː  | E:     |                              |',\n",
       " '| ch       | ç   | C      |                              |',\n",
       " '| t        | t   | t      |                              |',\n",
       " '| e        | e   | e      |                              |',\n",
       " '| l        | l   | l      |                              |',\n",
       " '| n        | n   | n      |                              |']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more data/german-orthography-profile.tsv | csvlook -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command line `segments` function and passing it our orthography profile, we can now segment our German text example into graphemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"IPA\" in table None',\n",
       " '  \\'Unspecified column \"{0}\" in table {1}\\'.format(k, self.local_name))',\n",
       " '/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"XSAMPA\" in table None',\n",
       " '  \\'Unspecified column \"{0}\" in table {1}\\'.format(k, self.local_name))',\n",
       " '/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"COMMENT\" in table None',\n",
       " '  \\'Unspecified column \"{0}\" in table {1}\\'.format(k, self.local_name))',\n",
       " 'Sch ä ch t e l ch e n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/german.txt | segments --profile=data/german-orthography-profile.tsv tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by providing `segments` a column for transliteration, we can convert the text into IPA or XSAMPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"IPA\" in table None',\n",
       " '  \\'Unspecified column \"{0}\" in table {1}\\'.format(k, self.local_name))',\n",
       " '/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"XSAMPA\" in table None',\n",
       " '  \\'Unspecified column \"{0}\" in table {1}\\'.format(k, self.local_name))',\n",
       " '/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"COMMENT\" in table None',\n",
       " '  \\'Unspecified column \"{0}\" in table {1}\\'.format(k, self.local_name))',\n",
       " 'ʃ ɛː ç t e l ç e n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/german.txt | segments --mapping=IPA --profile=data/german-orthography-profile.tsv tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"IPA\" in table None',\n",
       " '  \\'Unspecified column \"{0}\" in table {1}\\'.format(k, self.local_name))',\n",
       " '/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"XSAMPA\" in table None',\n",
       " '  \\'Unspecified column \"{0}\" in table {1}\\'.format(k, self.local_name))',\n",
       " '/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"COMMENT\" in table None',\n",
       " '  \\'Unspecified column \"{0}\" in table {1}\\'.format(k, self.local_name))',\n",
       " 'S E: C t e l C e n']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/german.txt | segments --mapping=XSAMPA --profile=data/german-orthography-profile.tsv tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An additional example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we provide another example of working with orthography profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aäaaöaaüaa']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more sources/text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['| Grapheme | mapping | frequency |',\n",
       " '| -------- | ------- | --------- |',\n",
       " '| a        | a       |         7 |',\n",
       " '| ä        | ä       |         1 |',\n",
       " '| ö        | ö       |         1 |',\n",
       " '| ü        | ü       |         1 |']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/text.txt | segments profile | csvlook -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/text.txt | segments profile > sandbox/orthography-profile.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Grapheme\\tmapping\\tfrequency', 'a\\ta\\t7', 'ä\\tä\\t1', 'ö\\tö\\t1', 'ü\\tü\\t1']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more sandbox/orthography-profile.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a ä a a ö a a ü a a']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/text.txt | segments tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"mapping\" in table None',\n",
       " '  \\'Unspecified column \"{0}\" in table {1}\\'.format(k, self.local_name))',\n",
       " '/usr/local/lib/python3.7/site-packages/csvw/metadata.py:704: UserWarning: Unspecified column \"frequency\" in table None',\n",
       " '  \\'Unspecified column \"{0}\" in table {1}\\'.format(k, self.local_name))',\n",
       " 'a ä a a ö a a ü a a']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cat sources/text.txt | segments --mapping=mapping --profile=sandbox/orthography-profile.tsv tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aäaaöaaüaa']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!more sources/text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
