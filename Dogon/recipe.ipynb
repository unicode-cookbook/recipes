{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Dogon comparative wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steven Moran &lt;bambooforest@gmail.com&gt;\n",
    "\n",
    "The latest version of this [Jupyter notebook](http://jupyter.org/) is available at [https://github.com/unicode-cookbook/recipes/Dogon](https://github.com/unicode-cookbook/recipes/Dogon). \n",
    "\n",
    "This use case illustrates how to tokenize a wordlist using an orthography profile. Details about orthography profiles and more is available in the [Unicode Cookbook for Linguists](https://github.com/unicode-cookbook/cookbook).\n",
    "\n",
    "This recipes uses Python 3.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Dogon and Bangime linguistics](http://dogonlanguages.org/) project collects and disseminates linguistic, cultural and geographic data from fieldwork undertaken on the Dogon languages, and the language isolate Bangime, spoken in central Mali. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data includes an extensive comparative [Dogon lexicon](https://github.com/clld/dogonlanguages-data) organized by the project members in an Excel spreadsheet. The columns include more than 20 lects (languages or dialects depending on the pair) spoken in Dogon country. There are English and French glosses for each row, e.g. 'cow that has calved at least once', 'vache qui a mis bas au moins une fois'. A row may also information in columns generated by the project, e.g. semantic domain (animal), subdomain (camel). Wordlist type, if there is any representative media file (especially flora and fauna), and so on. \n",
    "\n",
    "The Dogon comparative lexicon contains over 8000 rows. Sparsity is an issue in several languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparative wordlist was compiled by fieldworkers and each fieldworker has their own system for transcription. This will be made clear below when we create an initial orthography profile from the wordlist; it highlights idiosyncracies between transcription practives, e.g. there is both &lt;aa&gt; and &lt;aː&gt;.\n",
    "\n",
    "For this recipe, we will use a smaller curated version of the comparative Dogon wordlist available in the `sources` directory of this recipe. The wordlist is in CSV format with columns for a row ID, concept, doculect (language variety) and counterpart (word in the language).\n",
    "\n",
    "We will use the Python library [Pandas](http://pandas.pydata.org/) for the CSV reading and manipulation of the word list. The orthography profile for the Dogon lexical data is located in the `data` directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started import the [segments](https://pypi.python.org/pypi/segments/) and [Pandas](http://pandas.pydata.org/) modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from segments.tokenizer import Tokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the word list, for Pandas specify a row index column, and have a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 CONCEPT         DOCULECT COUNTERPART\n",
      "ID                                                   \n",
      "1   -teen ('11' to '19')          Ben_Tey        sâ:\n",
      "2   -teen ('11' to '19')        Dogul_Dom      sìgà\n",
      "3   -teen ('11' to '19')           Gourou      sáɣà\n",
      "4   -teen ('11' to '19')  Jamsay_Douentza      sáɣà\n",
      "5   -teen ('11' to '19')          Najamba      sìgà\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"sources/dogon-wordlist.tsv\", index_col=\"ID\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dogon orthography profile is in the `data` directory. Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grapheme        IPA     notes\n",
      "àà      àː\n",
      "áá      áː\n",
      "àá      ǎː\n",
      "áà      âː\n",
      "ààⁿ     ã̀ː\n",
      "ááⁿ     ã́ː\n",
      "àáⁿ     ã̌ː\n",
      "áàⁿ     ã̂ː\n",
      "ɔ̀ɔ̀      ɔ̀ː\n",
      "ɔ́ɔ́      ɔ́ː\n",
      "ɔ̀ɔ́      ɔ̌ː\n",
      "ɔ́ɔ̀      ɔ̂ː\n",
      "ɔ̀ɔ̀ⁿ     ɔ̃̀ː\n",
      "ɔ́ɔ́ⁿ     ɔ̃́ː\n",
      "ɔ̀ɔ́ⁿ     ɔ̃̌ː\n",
      "ɔ́ɔ̀ⁿ     ɔ̃̂ː\n",
      "ɛ̀ɛ̀      ɛ̀ː\n",
      "ɛ́ɛ́      ɛ́ː\n",
      "ɛ̀ɛ́      ɛ̌ː\n",
      "ɛ́ɛ̀      ɛ̂ː\n",
      "ɛ̀ɛ̀ⁿ     ɛ̃̀ː\n",
      "ɛ́ɛ́ⁿ     ɛ̃́ː\n",
      "\u001b[K\u001b[?1l\u001b>eath2016-profile.tsv\u001b[m\u001b[K\u0007"
     ]
    }
   ],
   "source": [
    "!more data/Heath2016-profile.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tokenizer object from the orthography profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = Tokenizer(\"data/Heath2016-profile.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add to our word list an additional column called `IPA` that will contain the output from orthography profile segmentation of the `COUNTERPART` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = lambda x: t.transform(x, column=\"IPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['TOKENS'] = pd.Series(df['COUNTERPART'].apply(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 CONCEPT         DOCULECT COUNTERPART     TOKENS\n",
      "ID                                                              \n",
      "1   -teen ('11' to '19')          Ben_Tey        sâ:      s âː\n",
      "2   -teen ('11' to '19')        Dogul_Dom      sìgà  s ì ɡ à\n",
      "3   -teen ('11' to '19')           Gourou      sáɣà  s á ɣ à\n",
      "4   -teen ('11' to '19')  Jamsay_Douentza      sáɣà  s á ɣ à\n",
      "5   -teen ('11' to '19')          Najamba      sìgà  s ì ɡ à\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the new segmented wordlist to the `sandbox` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('sandbox/segmented-dogon-wordlist.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an initial orthography profile from text input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have already provided you with a well-tested and curated orthography profile for the Dogon data. But if you want to create an initial profile from scratch, this is how you could do it from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from segments.tokenizer import Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next line is a bit cryptic. The `segments` Profile object takes as a parameter string text. Here we get the COUNTERPART column from the Pandas dataframe, which itself is a Pandas Series object. We first convert it to a list of string elements, which we join into one long string and feed it to the Profile function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "profile = Profile.from_text(''.join(df['COUNTERPART'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grapheme\tfrequency\tmapping\n",
      "n\t2457\tn\n",
      ":\t1935\t:\n",
      "à\t1816\tà\n",
      "á\t1778\tá\n",
      "m\t1648\tm\n",
      "g\t1608\tg\n",
      "ú\t1552\tú\n",
      "r\t1529\tr\n",
      "k\t1368\tk\n",
      "y\t1345\ty\n",
      "í\t1316\tí\n",
      "ɛ́\t1272\tɛ́\n",
      "l\t1223\tl\n",
      "d\t1199\td\n",
      "ù\t1193\tù\n",
      "b\t1171\tb\n",
      "ì\t1080\tì\n",
      "ⁿ\t1033\tⁿ\n",
      "ó\t986\tó\n",
      "ɛ̀\t970\tɛ̀\n",
      " \t914\t \n",
      "w\t914\tw\n",
      "é\t884\té\n",
      "ɔ́\t881\tɔ́\n",
      "ɔ̀\t826\tɔ̀\n",
      "s\t820\ts\n",
      "ŋ\t767\tŋ\n",
      "t\t750\tt\n",
      "è\t731\tè\n",
      "j\t696\tj\n",
      "ò\t621\tò\n",
      "p\t500\tp\n",
      "ǎ\t285\tǎ\n",
      "-\t255\t-\n",
      "ɲ\t237\tɲ\n",
      "c\t162\tc\n",
      "â\t155\tâ\n",
      "ɛ̌\t145\tɛ̌\n",
      "î\t143\tî\n",
      "ɔ̌\t104\tɔ̌\n",
      "=\t92\t=\n",
      "ʔ\t91\tʔ\n",
      "ɛ̂\t88\tɛ̂\n",
      "z\t87\tz\n",
      "ǐ\t84\tǐ\n",
      "ǹ\t81\tǹ\n",
      "ŋ̀\t80\tŋ̀\n",
      "û\t78\tû\n",
      "ǔ\t71\tǔ\n",
      "ɔ̂\t65\tɔ̂\n",
      "ǒ\t64\tǒ\n",
      "ě\t59\tě\n",
      "ê\t49\tê\n",
      "a\t47\ta\n",
      "ô\t46\tô\n",
      "m̀\t44\tm̀\n",
      "ỳ\t41\tỳ\n",
      "ɣ\t40\tɣ\n",
      "ɡ\t40\tɡ\n",
      "h\t38\th\n",
      "e\t36\te\n",
      "ń\t32\tń\n",
      "ɛ\t30\tɛ\n",
      "v́\t25\tv́\n",
      "ý\t25\tý\n",
      "ʒ\t24\tʒ\n",
      "ḿ\t21\tḿ\n",
      "o\t20\to\n",
      "f\t20\tf\n",
      "ẁ\t18\tẁ\n",
      "ʷ\t18\tʷ\n",
      "i\t16\ti\n",
      "ŋ́\t15\tŋ́\n",
      "ɔ\t14\tɔ\n",
      "v\t14\tv\n",
      "≡\t14\t≡\n",
      "→\t11\t→\n",
      "ɥ\t11\tɥ\n",
      "ð\t10\tð\n",
      "a᷈\t10\ta᷈\n",
      "ə̀\t10\tə̀\n",
      "ɔ᷈\t9\tɔ᷈\n",
      "r̃\t7\tr̃\n",
      "Y\t6\tY\n",
      "∴\t5\t∴\n",
      "v̀\t5\tv̀\n",
      "ʃ\t5\tʃ\n",
      "ə́\t5\tə́\n",
      "ʤ\t5\tʤ\n",
      "ɛ᷈\t4\tɛ᷈\n",
      "u\t4\tu\n",
      "w̃\t4\tw̃\n",
      "š\t4\tš\n",
      "ĩ́\t4\tĩ́\n",
      "ɪ́\t4\tɪ́\n",
      "o᷈\t4\to᷈\n",
      "ẃ\t3\tẃ\n",
      "…\t3\t…\n",
      "ɲ́\t3\tɲ́\n",
      "ʋ\t3\tʋ\n",
      "u᷈\t3\tu᷈\n",
      "ʸ\t2\tʸ\n",
      "ŷ\t2\tŷ\n",
      "/\t2\t/\n",
      "ĩ̀\t2\tĩ̀\n",
      "ɕ\t2\tɕ\n",
      "ɪ̀\t2\tɪ̀\n",
      "ɔ̯\t2\tɔ̯\n",
      "ⁿ́\t2\tⁿ́\n",
      "Ǹ\t1\tǸ\n",
      "ᵇ\t1\tᵇ\n",
      "N\t1\tN\n",
      "È\t1\tÈ\n",
      "'\t1\t'\n",
      "C\t1\tC\n",
      "ĺ\t1\tĺ\n",
      "(\t1\t(\n",
      "õ̀\t1\tõ̀\n",
      "ɔ̃́\t1\tɔ̃́\n",
      "i᷈\t1\ti᷈\n",
      "ɛ̄\t1\tɛ̄\n",
      ";\t1\t;\n",
      "ṍ\t1\tṍ\n",
      "ɔ̃̀\t1\tɔ̃̀\n",
      "ɲ̀\t1\tɲ̀\n",
      "ⁿ̀\t1\tⁿ̀\n",
      "\b\t1\t\b\n",
      ".\t1\t.\n",
      "e᷈\t1\te᷈\n"
     ]
    }
   ],
   "source": [
    "print(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The initial orthography profile is simply a unigram model with graphemes and their frequencies. You can open the file in a text editor and define tailored grapheme clusters and their mappings for an even more powerful orthography profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
